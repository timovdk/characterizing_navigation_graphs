{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utm\n",
    "import os\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_end_point(lat, lon):\n",
    "    start = [lon - 55, lat + 50]\n",
    "    end = [lon + 55, lat - 50]\n",
    "    if start[0] > 180:\n",
    "        start[0] = start[0] - 360\n",
    "    elif start[0] < -180:\n",
    "        start[0] = start[0] + 360\n",
    "    if start[1] > 80:\n",
    "        start[1] = start[1] - 160\n",
    "    elif start[1] < - 80:\n",
    "        start[1] = start[1] + 160\n",
    "\n",
    "    if end[0] > 180:\n",
    "        end[0] = end[0] - 360\n",
    "    elif end[0] < -180:\n",
    "        end[0] = end[0] + 360\n",
    "    if end[1] > 80:\n",
    "        end[1] = end[1] - 160\n",
    "    elif end[1] < - 80:\n",
    "        end[1] = end[1] + 160\n",
    "\n",
    "    return(start, end)\n",
    "\n",
    "def conv_func(data): # dont do this, ever, this is very bad. Will need to rewrite it for sure. NEED TO RE DO THIS FOR SURE\n",
    "\n",
    "    starting_points = []\n",
    "    ending_points = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        start, end = start_end_point(data.iloc[i]['latitude'], data.iloc[i]['longitude'])\n",
    "        \n",
    "        start_utm = utm.from_latlon(start[1], start[0])\n",
    "        starting_points.append(start_utm[2:])\n",
    "        \n",
    "        end_utm = utm.from_latlon(end[1], end[0])\n",
    "        ending_points.append(end_utm[2:])\n",
    "\n",
    "    starting_zones = pd.DataFrame(starting_points)\n",
    "    ending_zones = pd.DataFrame(ending_points)\n",
    "\n",
    "    coords = pd.concat([data, starting_zones, ending_zones],axis=1)\n",
    "    coords.columns = ['latitude', 'longitude', 's_zone', 's_band', 'e_zone', 'e_band']\n",
    "\n",
    "    coords['ns_band'] = [ ord(x) - 64 for x in coords.s_band ]\n",
    "    coords['ne_band'] = [ ord(x) - 64 for x in coords.e_band ]\n",
    "\n",
    "    bins_zone = [i*15 for i in range(0,5)]\n",
    "    labels_zone = [i for i in range(1,5)]\n",
    "\n",
    "    bins_band = [i*6 for i in range(0,5)]\n",
    "    labels_band = [i for i in range(1,5)]\n",
    "\n",
    "    # Binning results\n",
    "    coords['s_bin_zone'] = pd.DataFrame(pd.cut(coords['s_zone'], bins=bins_zone, labels=labels_zone))\n",
    "    coords['s_bin_band'] = pd.DataFrame(pd.cut(coords['ns_band'], bins=bins_band, labels=labels_band))\n",
    "    coords['e_bin_zone'] = pd.DataFrame(pd.cut(coords['e_zone'], bins=bins_zone, labels=labels_zone))\n",
    "    coords['e_bin_band'] = pd.DataFrame(pd.cut(coords['ne_band'], bins=bins_band, labels=labels_band))\n",
    "\n",
    "    return coords\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = [f for f in os.listdir('./data/head_tracking_data') if 'Subject' in f] # list of all subjects\n",
    "video_list = [f for f in os.listdir('./data/video_files') if 'mp4' in f] # list of all videos\n"
   ]
  },
  {
   "source": [
    "### Segmented data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = {} # each video will contain all subjects\n",
    "\n",
    "frames_per_segment = 30\n",
    "# video_id = video_list[19]\n",
    "for video_id in video_list:  # << thats the part that did for all videos\n",
    "    subjects = {} # for each subject, we store segments which are in seperate lists\n",
    "\n",
    "    for subject_id in subject_list:\n",
    "        tmp_data = pd.read_fwf('data/head_tracking_data/'+ subject_id + '/' + video_id.split('.mp4')[0] + '.txt', header=None)\n",
    "        tmp_data.columns = ['latitude', 'longitude']\n",
    "        processed_data = conv_func(tmp_data)\n",
    "        n_segments = int(len(tmp_data)/frames_per_segment) # ofc its not entirely coorect yet, it approximates how many segments\\\n",
    "        segments = []\n",
    "\n",
    "        for seg in range(n_segments):\n",
    "            start = frames_per_segment * seg\n",
    "            end = frames_per_segment * (seg+1)\n",
    "            segments.append(processed_data[start:end])\n",
    "\n",
    "        subjects[subject_id] = segments\n",
    "\n",
    "    videos[video_id] = subjects"
   ]
  },
  {
   "source": [
    "### Non segmented data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_wo_segments = {} # each video will contain all subjects\n",
    "\n",
    "frames_per_segment = 30\n",
    "for video_id in video_list:  # << thats the part that did for all videos\n",
    "    subjects = {} # for each subject, we store segments which are in seperate lists\n",
    "\n",
    "    for subject_id in subject_list:\n",
    "        tmp_data = pd.read_fwf('data/head_tracking_data/'+ subject_id + '/' + video_id.split('.mp4')[0] + '.txt', header=None)\n",
    "        tmp_data.columns = ['latitude', 'longitude']\n",
    "        processed_data = conv_func(tmp_data)\n",
    "\n",
    "        subjects[subject_id] = processed_data\n",
    "\n",
    "    videos_wo_segments[video_id] = subjects"
   ]
  },
  {
   "source": [
    "### For big navigation graph, figures out within a segment the unique transitions (has to use segmented data)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = {}\n",
    "for video in video_list:\n",
    "    many_users = {}\n",
    "    # union of unique tiles within the segment, so we can connect the segments in graph\n",
    "    for user in subject_list:\n",
    "        single_user = pd.DataFrame()\n",
    "        for segment in videos[video][user]:\n",
    "            \n",
    "            tmp_segment = segment.reset_index(drop=True)\n",
    "            lst = []\n",
    "            for i in range(len(tmp_segment)):\n",
    "                lst.append(((tmp_segment.iloc[i]['s_bin_zone'], tmp_segment.iloc[i]['s_bin_band']), (tmp_segment.iloc[i]['e_bin_zone'], tmp_segment.iloc[i]['e_bin_band'])))\n",
    "\n",
    "            df = pd.DataFrame(lst)\n",
    "            df.columns = ['s', 'e']\n",
    "\n",
    "            uniq_correct = df.groupby(['s','e']).size().reset_index().rename(columns={0:'count'})\n",
    "            uniq_correct = uniq_correct.sort_values(by=['s', 'e'])\n",
    "\n",
    "            single_user = single_user.append({'unique': str(uniq_correct[['s','e']].values)},ignore_index=True)\n",
    "\n",
    "        many_users[user] = single_user\n",
    "        \n",
    "    full_data[video] = many_users"
   ]
  },
  {
   "source": [
    "### Generates single user navigation graphs, stores it in a dictionary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_videos = {}\n",
    "\n",
    "for video_id in video_list:\n",
    "    stats_subj = {}\n",
    "    for subj_id in subject_list:\n",
    "        G = nx.DiGraph()\n",
    "        ti = pd.DataFrame()\n",
    "        for i in range(len(videos_wo_segments[video_id][subj_id])-1):\n",
    "\n",
    "            prev_top_left_zone = (videos_wo_segments[video_id][subj_id].iloc[i]['s_bin_zone'], videos_wo_segments[video_id][subj_id].iloc[i]['s_bin_band'])\n",
    "            prev_bottom_right_zone = (videos_wo_segments[video_id][subj_id].iloc[i]['e_bin_zone'], videos_wo_segments[video_id][subj_id].iloc[i]['e_bin_band'])\n",
    "            curr_top_left_zone = (videos_wo_segments[video_id][subj_id].iloc[i+1]['s_bin_zone'], videos_wo_segments[video_id][subj_id].iloc[i+1]['s_bin_band'])\n",
    "            curr_bottom_right_zone = (videos_wo_segments[video_id][subj_id].iloc[i+1]['e_bin_zone'], videos_wo_segments[video_id][subj_id].iloc[i+1]['e_bin_band'])\n",
    "\n",
    "            G.add_edge((prev_top_left_zone, prev_bottom_right_zone), (curr_top_left_zone, curr_bottom_right_zone))\n",
    "            \n",
    "        stats_subj[subj_id] = G\n",
    "\n",
    "    all_videos[video_id] = stats_subj\n"
   ]
  },
  {
   "source": [
    "### Save data to pickle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_obj(full_data, 'data')\n",
    "# save_obj(videos_wo_segments, 'data_wo_segments')\n",
    "# save_obj(all_videos, 'single_user_ngs(all)')"
   ]
  }
 ]
}